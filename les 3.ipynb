{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.875     ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.80000001  0.375     ]\n",
      " [ 0.2         1.        ]\n",
      " [ 0.40000001  1.        ]\n",
      " [ 0.89999998  0.125     ]\n",
      " [ 0.60000002  1.        ]\n",
      " [ 0.1         0.875     ]\n",
      " [ 0.5         0.375     ]\n",
      " [ 0.69999999  0.75      ]]\n",
      "[[ 1.        ]\n",
      " [ 0.73195875]\n",
      " [ 0.76288658]\n",
      " [ 0.8041237 ]\n",
      " [ 0.92783505]\n",
      " [ 0.70103091]\n",
      " [ 0.97938144]\n",
      " [ 0.67010307]\n",
      " [ 0.72164947]\n",
      " [ 0.84536082]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input1=[[10,7],[5,4],[8,3],[2,8],[4,8],[9,1],[6,8],[1,7],[5,3],[7,6]]\n",
    "output1=[[97],[71],[74],[78],[90],[68],[95],[65],[70],[82]]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input = tf.placeholder(tf.float32)\n",
    "    output = tf.placeholder(tf.float32)\n",
    "\n",
    "    max_input = tf.reduce_max(input,0)\n",
    "    max_output = tf.reduce_max(output,0)\n",
    "\n",
    "    normalized_input = input / max_input\n",
    "    normalized_output = output / max_output\n",
    "\n",
    "    wights_1 = tf.Variable(tf.zeros([2,3]))\n",
    "\n",
    "    wighted_sum = tf.matmul(normalized_input,wights_1)\n",
    "    activation_1 = tf.sigmoid(wighted_sum)\n",
    "    \n",
    "    wights_2 = tf.Variable(tf.zeros([3,1]))\n",
    "    \n",
    "    wighted_sum_2 = tf.matmul(normalized_input,wights_2)\n",
    "    activation_2 = tf.sigmoid(wighted_sum)\n",
    "\n",
    "    with tf.Session() as se:\n",
    "        print(se.run(normalized_input, feed_dict = {input:input1,output:output1}))\n",
    "        print(se.run(normalized_output, feed_dict = {input:input1,output:output1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.875     ]\n",
      " [ 0.5         0.5       ]\n",
      " [ 0.80000001  0.375     ]\n",
      " [ 0.2         1.        ]\n",
      " [ 0.40000001  1.        ]\n",
      " [ 0.89999998  0.125     ]\n",
      " [ 0.60000002  1.        ]\n",
      " [ 0.1         0.875     ]\n",
      " [ 0.5         0.375     ]\n",
      " [ 0.69999999  0.75      ]]\n",
      "[ 1.          0.73195875  0.76288658  0.8041237   0.92783505  0.70103091\n",
      "  0.97938144  0.67010307  0.72164947  0.84536082]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "raw_input_data=[[10,7],\n",
    "                [5,4],\n",
    "                [8,3],\n",
    "                [2,8],\n",
    "                [4,8],\n",
    "                [9,1],\n",
    "                [6,8],\n",
    "                [1,7],\n",
    "                [5,3],\n",
    "                [7,6]]\n",
    "\n",
    "raw_output_data = [97,71,74,78,90,68,95,65,70,82]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input = tf.placeholder(tf.float32)\n",
    "    output = tf.placeholder(tf.float32)\n",
    "    \n",
    "    \n",
    "    max_input = tf.reduce_max(input,0)\n",
    "    max_output = tf.reduce_max(output,0)\n",
    "    \n",
    "    normalized_input = input / max_input\n",
    "    normalized_output = output / max_output\n",
    "    \n",
    "    weights_1 = tf.Variable(tf.zeros([2,3]))\n",
    "    bias_1 = tf.Variable(tf.zeros([1,3]))\n",
    "    weighted_sum_1 = tf.matmul(normalized_input, weights_1) + bias_1\n",
    "    \n",
    "    activation_1 = tf.sigmoid(weighted_sum_1)\n",
    "    \n",
    "    weights_2 = tf.Variable(tf.zeros([3,1]))\n",
    "    bias_2 = tf.Variable(tf.zeros([1]))\n",
    "    weighted_sum_2 = tf.matmul(activation_1, weights_2) + bias_2\n",
    "    \n",
    "    activation_2 = tf.sigmoid(weighted_sum_2)\n",
    "    \n",
    "    loss_1 = tf.reduce_sum((activation_2 - normalized_output)**2)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        print((sess.run(normalized_input, feed_dict = {input:raw_input_data,output:raw_output_data})))\n",
    "        \n",
    "        print((sess.run(normalized_output, feed_dict = {input:raw_input_data,output:raw_output_data})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_1 =\n",
      "[[ 2.30110574  0.45575598 -2.14336991]\n",
      " [ 2.85978556  1.89326799  0.28729439]]\n",
      "bias_1 =\n",
      "[-4.50302267 -1.34895146 -0.34414011]\n",
      "weights_2 =\n",
      "[[ 5.38227272]\n",
      " [ 1.83954048]\n",
      " [-1.8940922 ]]\n",
      "bias_2 =\n",
      "[-0.19885498]\n",
      "train data: normalized_output =\n",
      "[[ 1.        ]\n",
      " [ 0.73195875]\n",
      " [ 0.76288658]\n",
      " [ 0.8041237 ]\n",
      " [ 0.92783505]]\n",
      "train data: activation_2 =\n",
      "[[ 0.98139107]\n",
      " [ 0.71360326]\n",
      " [ 0.78103656]\n",
      " [ 0.82409614]\n",
      " [ 0.90533304]]\n",
      "train data: loss = 0.00038357527228072286\n",
      "test data: normalized_output =\n",
      "[[ 0.7157895 ]\n",
      " [ 1.        ]\n",
      " [ 0.68421054]\n",
      " [ 0.7368421 ]\n",
      " [ 0.86315787]]\n",
      "test data: activation_2 =\n",
      "[[ 0.73379475]\n",
      " [ 0.96387488]\n",
      " [ 0.70938361]\n",
      " [ 0.6768896 ]\n",
      " [ 0.93023098]]\n",
      "test data: loss = 0.002071200404316187\n",
      "unseen data: activation_2 =\n",
      "[[ 0.71446365]\n",
      " [ 0.71726012]\n",
      " [ 0.71360326]\n",
      " [ 0.7184931 ]\n",
      " [ 0.67393786]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Let's define our input and output data.\n",
    "# Note that this is all observed data, so it is defined as constants.\n",
    "# The data includes 5 examples and 2 features.\n",
    "# hours studied  |  hours slept  |  test score\n",
    "# ---------------------------------------------\n",
    "#       10       |      7        |     97\n",
    "#        5       |      4        |     71\n",
    "#        8       |      3        |     74\n",
    "#        2       |      8        |     78\n",
    "#        4       |      8        |     90\n",
    "#        9       |      1        |     68\n",
    "#        6       |      8        |     95\n",
    "#        1       |      7        |     65\n",
    "#        5       |      3        |     70\n",
    "#        7       |      6        |     82\n",
    "# ---------------------------------------------\n",
    "# Define the raw data that we have observed.\n",
    "train_input_data = [[10, 7], [5, 4], [8, 3], [2, 8], [4, 8]]\n",
    "train_output_data = [[97], [71], [74], [78], [90]]\n",
    "\n",
    "# Number of training examples.\n",
    "NUM_TRAIN_EXAMPLES = len(train_input_data)\n",
    "\n",
    "# Store the number of features so we can use it in our neural network structure.\n",
    "NUM_FEATURES = len(train_input_data[0])\n",
    "\n",
    "test_input_data = [[9, 1], [6, 8], [1, 7], [5, 3], [7, 6]]\n",
    "test_output_data = [[68], [95], [65], [70], [82]]\n",
    "\n",
    "# Number of training steps\n",
    "EPOCHS = 10000\n",
    "# Number of neurons in the neural network's hidden layer.\n",
    "NUM_HIDDEN_LAYER_NEURONS = 3\n",
    "# Learning rate of our gradient descent optimizer.\n",
    "LEARNING_RATE = 2.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Create a placeholder input tensor.\n",
    "  input = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "  # Create a placeholder for the output tensor (the observed test scores).\n",
    "  output = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "  train_data = {input: train_input_data, output: train_output_data}\n",
    "  test_data = {input: test_input_data, output: test_output_data}\n",
    "\n",
    "  # Determine the maximum value of each feature. The result is a 1x2 array.\n",
    "  max_input = tf.reduce_max(input, 0)\n",
    "\n",
    "  # Now all of our input values are in the range [0.0, 1.0].\n",
    "  normalized_input = input / max_input\n",
    "\n",
    "  # Set up the operations that will normalize the output tensor.\n",
    "  max_output = tf.reduce_max(output)\n",
    "  normalized_output = output / max_output\n",
    "\n",
    "  # Now let's describe the structure of the neural network. We have 2 input features,\n",
    "  # as described by the input dataset, so this corresponds to 2 nodes in the input\n",
    "  # layer. Now we want to have a single hidden layer with 3 neurons. Finally, the\n",
    "  # output layer will only have 1 neuron (the test score).\n",
    "  num_neurons_1 = NUM_FEATURES\n",
    "  num_neurons_2 = NUM_HIDDEN_LAYER_NEURONS\n",
    "  num_neurons_3 = 1\n",
    "\n",
    "  # Define the weights on the synapses between the input layer and the hidden layer.\n",
    "  weights_1 = tf.Variable(tf.random_normal([num_neurons_1, num_neurons_2]))\n",
    "\n",
    "  # Model the bias parameter, as well.\n",
    "  bias_1 = tf.Variable(tf.random_normal([num_neurons_2]))\n",
    "\n",
    "  # Compute the weighted sum and add in the bias. Note that bias_1 is 1x3, but the 3 \n",
    "  # values will be element-wise added to each row in weighted_sums_1, which is 7x3.\n",
    "  weighted_sums_1 = tf.matmul(normalized_input, weights_1) + bias_1\n",
    "\n",
    "  # Apply the activation function (sigmoid).\n",
    "  activation_1 = tf.sigmoid(weighted_sums_1)\n",
    "\n",
    "  # Do the same steps for the second (output) layer.\n",
    "  weights_2 = tf.Variable(tf.random_normal([num_neurons_2, num_neurons_3]))\n",
    "  bias_2 = tf.Variable(tf.random_normal([num_neurons_3]))\n",
    "  weighted_sums_2 = tf.matmul(activation_1, weights_2) + bias_2\n",
    "  activation_2 = tf.sigmoid(weighted_sums_2)\n",
    "\n",
    "  # Define the loss function as the sum of squared differences between observed\n",
    "  # and computed output.\n",
    "  loss = tf.reduce_sum((activation_2 - normalized_output)**2) / NUM_TRAIN_EXAMPLES\n",
    "\n",
    "  # Set up the Stochastic Gradient Descient optimizer to minimize the loss.\n",
    "  train_step = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(loss)\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Run EPOCHS steps of learning.\n",
    "    for i in range(EPOCHS):\n",
    "      sess.run(train_step, feed_dict = train_data)\n",
    "    \n",
    "    # Here are the trained weights and biases, in case we're interested.\n",
    "    print('weights_1 =\\n{}'.format(sess.run(weights_1)))\n",
    "    print('bias_1 =\\n{}'.format(sess.run(bias_1)))\n",
    "    print('weights_2 =\\n{}'.format(sess.run(weights_2)))\n",
    "    print('bias_2 =\\n{}'.format(sess.run(bias_2)))\n",
    "\n",
    "    # Does our trained model perform well on the training data?\n",
    "    print('train data: normalized_output =\\n{}'.format(sess.run(normalized_output, feed_dict = train_data)))\n",
    "    print('train data: activation_2 =\\n{}'.format(sess.run(activation_2, feed_dict = train_data)))\n",
    "\n",
    "    # A good measure of how well we're doing is how small the loss is.\n",
    "    print('train data: loss = {}'.format(sess.run(loss, feed_dict = train_data)))\n",
    "\n",
    "    # Does our trained model perform well on the testing data?\n",
    "    print('test data: normalized_output =\\n{}'.format(sess.run(normalized_output, feed_dict = test_data)))\n",
    "    print('test data: activation_2 =\\n{}'.format(sess.run(activation_2, feed_dict = test_data)))\n",
    "\n",
    "    # A good measure of how well we're doing is how small the loss is.\n",
    "    print('test data: loss = {}'.format(sess.run(loss, feed_dict = test_data)))      \n",
    "\n",
    "    # Run it on a couple of new inputs just to see what the model predicts.\n",
    "    print('unseen data: activation_2 =\\n{}'.format(sess.run(activation_2, feed_dict = {input: [[0, 8], [1, 7], [4, 4], [2, 6], [8, 0]]})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
